---
title: "hw1_5"
author: "Mahdiazhari Austian"
date: "5/9/2022"
output: html_document
---

# Homework 1 - Number 5 - Forecasting with Gaussian Process 

Prompt:
Predict the SP500 with the financial indicators selected by
your team in the google spreadsheet (ep, dp, de, dy, dfy, bm, svar, ntis, infl, tbl , see
RLab3 GWcausalSP500.R), some lagged series of these indicators and lags of the target
using a GP regression with your desired kernel. Predict return, or price, or trend (for which
target works best?) select appropriate kernel and justify its use.

Do some feature selection to disregard some variables, select appropriate lags: 
causality, (distance) correlation, VaR-test, Lasso


## Import Packages
```{r}
fileloc <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(fileloc)
rm(fileloc)
library(kernlab) ##for GP
library(MASS)
#library(Metrics)##Measures of prediction error:mse, mae
library(xts) ## handling time series
library(reshape2)
library(ggplot2)
library(plyr)
library("quantmod")
library("vars")
library("lmtest") #comes with vars
library('imputeTS') #for imputing time series
library(glmnet) #for variable selection phase
library('HDeconometrics') #for the lasso BIC functions
set.seed(1996)
```


## Import variables, transform predicted variable

```{r}
sp500 = as.xts(read.zoo('data/GoyalMonthly2005.csv',sep=',',header=TRUE, format='%Y-%m-%d'))
names(sp500)
mt=sp500['1927/2005']
```



```{r}
head(mt)
```



```{r}
##frequency of sampling
tau=1 #data is monthly. Try tau=12 (year), tau=3 (quarterly)

```

### Set the Target Variable

Set the target as log returns
```{r}
##1. Target and Feature as plain Price
target <- mt$Index

##2. Target and Feature as Returns
#sp500PE = diff(log(sp500PE),diff=tau)
target= diff(log(target),diff=tau)  ##compute tau-period returns
##try non-center (above) /center target (below)
target=na.trim(target-mean(na.omit(target))) ##Center the target 
```


Calculate predictor variables as given in Rlab3_GWCausal500.R
```{r}
##Predictor (wanna-be) variables
# dividend-price ratio (dp)
dp <- log(mt$D12) - log(mt$Index)
# dividend-payout ratio (de)
de <- log(mt$D12) - log(mt$E12)
# earnings to price
ep <- log(mt$E12) - log(mt$Index)
## dividend yield 
dy <- log(mt$D12) - log(stats::lag(mt$Index,1))
# Default yield spread (dfy)= BAA-AAA rated corporate bond yields:
dfy <- mt$BAA -mt$AAA

```


Set other variables here
```{r}
## from the table consider stock variance (svar), Book-to-Market (b.m)
## net equity expansion (ntis, start 1926), inflation (infl)
# Treasury Bill rates (tbl, 1920)
svar = mt$svar
bm <-mt$b.m
ntis <- mt$ntis
infl <-mt$infl
tbl <- mt$tbl

names(GSPCep) = "GSPCep"; 
names(ep) = "ep"; names(bm) = "bm"   
names(dp) = "dp"; names(svar) = "svar"  
names(dy) = "dy"; names(de) = "de"   
names(ntis) = "ntis"; names(infl) = "infl"  
names(tbl) = "tbl"; names(dfy) ="dfy"
```


## Feature Matrix Creation

Create a matrix of features consisting of the target and its lags plus the indicators and their lags:
```{r}
##Model Inputs:
##Define matrix of features (each column is a feature)
#Features: lags 1,2,3 with (or w/o) ep,svar,bm and lags 1,2
feat = merge(na.trim(stats::lag(target,1)),na.trim(stats::lag(target,2)),na.trim(stats::lag(target,3)),
             bm,ep,svar,na.trim(stats::lag(bm,1)),na.trim(stats::lag(bm,2)),na.trim(stats::lag(ep,1)), na.trim(stats::lag(ep,2)), na.trim(stats::lag(svar,1)), na.trim(stats::lag(svar,2)),
             #add other features here,
             all=FALSE)
```


Merge the feature matrix with indicators
```{r}
##add TARGET. We want to predict PRICE or RETURN
dataset = merge(feat,target,all=FALSE)
colnames(dataset) = c("lag.1", "lag.2", "lag.3",
                      "bm","ep","svar", "bm.1", "bm.2", "ep.1", "ep.2", "svar.1", "svar.2",
                      #names of other features,
                      "TARGET")
```


```{r}
head(dataset)
```


```{r}
##Divide data into training (75%) and testing (25%). 
T<-nrow(dataset)
p=0.75
T_trn <- round(p*T)
trainindex <- 1:T_trn
##process class sets as data frames
training = as.data.frame(dataset[trainindex,])
rownames(training) = NULL
testing = as.data.frame(dataset[-trainindex,])
rownames(testing) = NULL
```


## Variable Selection with LASSO

For this assignment, we will use LASSO BIC to do Feature Selection. This will 
be implemented using the glmnet package in the HDEconometrics repository.

source: 
https://www.r-bloggers.com/2017/04/lasso-adalasso-and-the-glmnet-package/
https://github.com/gabrielrvsc/HDeconometrics

##LASSO_BIC

```{r}
x <- training[,1:12] #separating the intercept
head(x)
```

```{r}
y <- training[,"TARGET"] #put the independent variable into y
```

```{r}
head(y)
```

The following plot below shows the variables going to zero as 
the penalty increases in the objective function of the LASSO. 

```{r}
lasso=ic.glmnet(x,y,crit = "bic")
plot(lasso$glmnet,"lambda",ylim=c(-2,2))

```

The plot below shows the BIC curve and the selected model (the lines).

```{r}
plot(lasso)
```

We found that LASSO selects 11 variables out of 12.

## Find the Variables to Drop


```{r}
b.bic.lasso= lasso$coef
```

```{r}
names(b.bic.lasso)
```

```{r}
sum(b.bic.lasso == 0) 
sum(b.bic.lasso != 0)
```



```{r}
print(b.bic.lasso == 0)
```

We can see that bm (book-to-market lagged 2 months) is the variable to drop.


```{r}
dataset2= select(dataset, -'bm.2')
```


# GP Prediction

## Kernels 

### Kernel from RLAB3_GPLab

Use given kernel
```{r}
## GP Kernel: vanilla is non-stationary; tanh, rbf are stationary
## Define my own Kernel (for vectors). You have 3 choices below:
##SE kernel with length scale l


#l=0.003 ##0.15, 0.3
# MyKer <- function(x,y) {
 # 1.5*exp((sum((x-y)^2))/(-2*l^2))
# }

 MyKer <- function(x,y) {
  2*exp(sum(abs(x-y))/(-2*1.5^2)) + 1.5*sum(x*y)
}

class(MyKer) <- 'kernel'
```



```{r}
gpfit = gausspr(TARGET~., data=training,
                type="regression",
                #kernel="tanhdot", 
                #kernel= "rbfdot", #"vanilladot",
                kernel= MyKer,  
                #kpar = list(sigma = 0.4), #list of kernel hyper-parameters for rbf
                ## if you make it constant value then does not make mle estimation of sigma
                #kpar=list(scale=2,offset=2), ##for tanh
                var = 0.003 # the initial noise variance: 0.001 default min value
)
gpfit

##build predictor (predict on test data)
GPpredict <- predict(gpfit,testing)
```


### RBF Kernel

```{r}
gpfit.rbf = gausspr(TARGET~., data=training,
                type="regression",
                #kernel="tanhdot", 
                kernel= "rbfdot", #"vanilladot",
                #kernel= MyKer,  
                #kpar = list(sigma = 0.4), #list of kernel hyper-parameters for rbf
                ## if you make it constant value then does not make mle estimation of sigma
                #kpar=list(scale=2,offset=2), ##for tanh
                var = 0.003 # the initial noise variance: 0.001 default min value
)
gpfit.rbf

##build predictor (predict on test data)
GPpredict.rbf <- predict(gpfit.rbf,testing)
```

### Vanilladot Kernel

```{r}
gpfit.vanilla = gausspr(TARGET~., data=training,
                type="regression",
                #kernel="tanhdot", 
                kernel= "vanilladot", #"vanilladot",
                #kernel= MyKer,  
                #kpar = list(sigma = 0.4), #list of kernel hyper-parameters for rbf
                ## if you make it constant value then does not make mle estimation of sigma
                #kpar=list(scale=2,offset=2), ##for tanh
                var = 0.003 # the initial noise variance: 0.001 default min value
)
gpfit.vanilla

##build predictor (predict on test data)
GPpredict.vanilla <- predict(gpfit.vanilla,testing)
```




##EVALUATION Functions


```{r}
##sum of squares errors function
ssr <-function(actual,pred){
  sum((actual - pred)^2)
}
```



```{r}
##Normalize Residual Mean Square Error (NRMSE) funct
nrmse <- function(actual,pred){
  sqrt(ssr(actual,pred)/((length(actual)-1)*var(actual)))
} 
```


```{r}
##percentage of outperforming direct sample mean (sample expected value)
pcorrect<- function(actual,pred){
  (1-nrmse(actual,pred))*100
}
```


##1. Evaluation for TARGET prediction. Evaluating SE Kernel 

```{r}
### Evaluation of Results
actualTS=testing[,ncol(testing)] ##the true series to predict
predicTS = GPpredict

res <- list("GP"=pcorrect(actualTS,predicTS))
unlist(res)
```

```{r}

##Check the gap between training error (mse) and testing error
train.error <- error(gpfit)  
testing.error <- mean((actualTS - predicTS)^2)
gap <- testing.error - train.error ; gap


```

```{r}

##For visual comparison

yl=c(min(actualTS,predicTS),max(actualTS,predicTS)) #set y limits
plot(actualTS,predicTS,ylim=yl)

##Forecasting: PRICE, draw the simulations of price
#par( mfrow = c( 1, 2 ) )
#par(mar=c(2.5,2.5,2.5,2.5))
plot(actualTS,t='l',col='gray20', ylab='', xlab ='',lty=3, main='GP predictions', cex.main=0.75)
lines(GPpredict,col='green',lwd=2)
legend('bottomright',legend = c('target','GP'),col=c('gray20','green'),lty=c(3,1),cex=.7)
```
##2. Evaluation for TARGET prediction. Evaluating RBF Kernel 

```{r}
#actualTS=testing[,ncol(testing)] ##the true series to predict
predicTS.rbf = GPpredict.rbf

res.rbf <- list("GP.rbf"=pcorrect(actualTS,predicTS.rbf))
unlist(res.rbf)
```



Check Gap:
```{r}
train.error.rbf <- error(gpfit.rbf)  
testing.error.rbf <- mean((actualTS - predicTS.rbf)^2)
gap.rbf <- testing.error.rbf - train.error.rbf ; gap.rbf
```




```{r}
yl=c(min(actualTS,predicTS.rbf),max(actualTS,predicTS.rbf)) #set y limits
plot(actualTS,predicTS.rbf,ylim=yl)

##Forecasting: PRICE, draw the simulations of price
#par( mfrow = c( 1, 2 ) )
#par(mar=c(2.5,2.5,2.5,2.5))
plot(actualTS,t='l',col='gray20', ylab='', xlab ='',lty=3, main='GP predictions with RBF Kernel', cex.main=0.75)
lines(GPpredict.rbf,col='green',lwd=2)
legend('bottomright',legend = c('target','GP'),col=c('gray20','green'),lty=c(3,1),cex=.7)
```


##3. Evaluation for TARGET prediction. Evaluating Vanilladot Kernel 





